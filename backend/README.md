# RFQ Deep Agent â€“ Conversational RFQ Generator

An intelligent RFQ Assistant built using **FastAPI + LLM + Retrieval System** to:
- Validate automotive domain requirements
- Search relevant RFQs using hybrid search
- Load RFQ templates
- Conversationally guide users
- Perform RFQ Impact Analysis
- Auto-apply recommended changes
- Generate Strict OEM-Standard RFQs
- Export Professionally Structured RFQ as PDF / DOCX

This is NOT a toy chatbot.  
This behaves like a professional RFQ expert.

---

## âœ¨ Features

### âœ”ï¸ Conversational RFQ Assistant
- Welcomes user dynamically based on time
- Understands natural language RFQ needs
- Asks clarification questions only when required

### âœ”ï¸ Domain Validation
Checks if the requirement is actually Automotive related.

### âœ”ï¸ RFQ Retrieval
Uses Hybrid Search to retrieve best matching RFQs.

### âœ”ï¸ RFQ Editing + Impact Analysis
Whenever user edits something:
- Identifies WHAT changed
- Detects DEPENDENT SECTIONS
- Gives ACTIONABLE RECOMMENDATIONS
- No unnecessary rewriting

### âœ”ï¸ Intelligent RFQ Auto-Drafting
When user explicitly asks for:
- *apply changes*
- *generate final structured rfq*
- *update rfq*
- *create final rfq*

LLM switches into **STRICT OEM RFQ Mode** and generates a highly professional structured RFQ.

### âœ”ï¸ Clean RFQ Text Engine
Fixes formatting issues:
- Removes junk text
- Ensures headings spacing
- Proper bullet structure
- Clean sections

### âœ”ï¸ Export Engine
Exports perfectly formatted:
- PDF (ReportLab)
- DOCX

With:
- Cover Page
- Table of Contents
- Section Formatting
- Revision History
- Page Headers & Footers

---

## ğŸ—ï¸ Project Structure

```
backend/
â”œâ”€â”€ api/                    # API route handlers
â”‚   â”œâ”€â”€ documents.py       # Document management endpoints
â”‚   â”œâ”€â”€ generator.py       # RFQ generation endpoints
â”‚   â””â”€â”€ rfqs.py           # RFQ management endpoints
â”œâ”€â”€ core/                  # Core business logic
â”‚   â”œâ”€â”€ embedding_model.py # Text embedding utilities
â”‚   â”œâ”€â”€ image_processor.py # Image classification & embedding
â”‚   â”œâ”€â”€ ingestion.py      # Document ingestion pipeline
â”‚   â”œâ”€â”€ llm_agent.py      # Conversational agent logic
â”‚   â”œâ”€â”€ llm_provider.py   # Multi-provider LLM support
â”‚   â”œâ”€â”€ prompt_loader.py  # Prompt template management
â”‚   â”œâ”€â”€ retriever.py      # Hybrid search implementation
â”‚   â””â”€â”€ text_utils.py     # Text processing utilities
â”œâ”€â”€ prompts/              # LLM prompt templates
â”œâ”€â”€ data/                 # Document storage (gitignored)
â”œâ”€â”€ exports/              # Generated RFQ exports (gitignored)
â”œâ”€â”€ database.py           # PostgreSQL + pgvector connection
â”œâ”€â”€ logging_config.py     # Centralized logging setup
â”œâ”€â”€ main.py              # FastAPI application entry point
â”œâ”€â”€ render.py            # PDF/DOCX rendering engine
â”œâ”€â”€ settings.py          # Configuration management
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.10+
- PostgreSQL with pgvector extension
- Docker (for database)

### 1. Start Database
```bash
docker-compose up -d
```

### 2. Install Dependencies
```bash
cd backend
pip install -r requirements.txt
```

### 3. Configure Environment
Copy `.env.example` to `.env` and configure:
```bash
cp .env.example .env
```

Required settings:
- `LLM_API_KEY` - Your Groq/OpenAI/Anthropic API key
- `HUGGINGFACE_TOKEN` - For image models (optional)
- `POSTGRES_*` - Database credentials (default works with docker-compose)

### 4. Run Server
```bash
python -m uvicorn main:app --host 127.0.0.1 --port 8000 --reload
```

API will be available at: http://127.0.0.1:8000  
API Documentation: http://127.0.0.1:8000/docs

---

## ğŸ§ª Testing Ideas

Try these conversational cases:
- *need rfq for steering components*
- *change delivery timeline to 3 weeks*
- *apply the recommended changes*
- *generate final structured rfq*
- *auto update dependent sections*
- *export as pdf*

---

## âš™ï¸ Configuration

### LLM Providers
Supports multiple providers via `.env`:
- **Groq** (default, fast and free)
- **OpenAI** (GPT-4, GPT-3.5)
- **Anthropic** (Claude)
- **Google** (Gemini)

### Image Models
- **Primary**: `jinaai/jina-clip-v1` (configurable)
- **Fallback**: `openai/clip-vit-base-patch32`

Change via `IMAGE_MODEL_NAME` in `.env`

---

## âš ï¸ Known Constraints
- Only supports AUTOMOTIVE domain (by design)
- Requires internet for LLM responses
- Must not manually break formatting logic

---

## ğŸ Status
Actively Functional  
Used for real RFQ conversational automation  
Not a demo toy.

---

## ğŸ¤ Credits
Built with:
- FastAPI
- Groq LLM
- PostgreSQL + pgvector
- Sentence Transformers
- JinaCLIP
- ReportLab
- Python-Docx
